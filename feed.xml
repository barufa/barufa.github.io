<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://barufa.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://barufa.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-07T23:09:07+00:00</updated><id>https://barufa.github.io/feed.xml</id><title type="html">StuckInALocalMinima</title><subtitle>I’m Bruno — a CS grad turned **Senior Machine Learning Engineer** who loves building things that run fast, scale well, and sometimes even work on the first try. Currently into deep learning, edge devices, and squeezing performance out of both code and coffee. ☕🤖 </subtitle><entry><title type="html">From scikit-learn to Faiss: Migrating PCA for Scalable Vector Search</title><link href="https://barufa.github.io/blog/2025/sklearn-faiss/" rel="alternate" type="text/html" title="From scikit-learn to Faiss: Migrating PCA for Scalable Vector Search"/><published>2025-07-19T00:00:00+00:00</published><updated>2025-07-19T00:00:00+00:00</updated><id>https://barufa.github.io/blog/2025/sklearn-faiss</id><content type="html" xml:base="https://barufa.github.io/blog/2025/sklearn-faiss/"><![CDATA[<h2 id="why-using-faiss">Why using Faiss</h2> <p>Faiss is a high‑performance library for vector similarity search and related primitives (clustering, compression, linear transforms like PCA). t scales to millions–billions of vectors on CPU and GPU and it is a much faster implementation of PCA. In practice this reduces memory, latency, and Python overhead.</p> <h3 id="why-migrate-pca-to-faiss">Why migrate PCA to Faiss?</h3> <p>If you’re already using scikit-learn for training, why switch to Faiss for deployment?</p> <ul> <li>Training PCA in sklearn is convenient, but for the deployment implementation is slow.</li> <li>Faiss offers faster, more efficient kernels for applying PCA at scale.</li> <li>You can migrate a trained sklearn.PCA to a faiss.PCAMatrix without retraining.</li> </ul> <h2 id="principal-component-analysis">Principal Component Analysis</h2> <p>PCA (Principal Component Analysis) is a linear dimensionality reduction technique. It projects data into a lower-dimensional space using the eigenvectors of the covariance matrix. You can check <a href="https://youtu.be/dhK8nbtii6I?si=rEa2z5YDaGERLTfy">this video</a> for a detail exaplanation.</p> <h3 id="sklearn">Sklearn</h3> <p>We’ll focus on the essential operation of PCA: projecting vectors using <code class="language-plaintext highlighter-rouge">transform()</code>.</p> <p>Given :</p> <ul> <li>X as the input data</li> <li>skl_pca as the trained PCA object from sklearn</li> </ul> <p>You can project X into the PCA-transformed space like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_transformed = X @ skl_pca.components_.T
</code></pre></div></div> <p>If whitening was applied during PCA fitting, you’ll also need to scale the output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scale = xp.sqrt(skl_pca.explained_variance_)
min_scale = xp.finfo(scale.dtype).eps
scale[scale &lt; min_scale] = min_scale
X_transformed /= scale
</code></pre></div></div> <p>For reference, see the <a href="https://github.com/scikit-learn/scikit-learn/blob/c5497b7f7/sklearn/decomposition/_base.py#L116">official implementation</a>.</p> <h3 id="faiss">Faiss</h3> <p>In Faiss, after training a <code class="language-plaintext highlighter-rouge">PCAMatrix</code>, the transformation looks slightly different:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> X_transformed = X @ faiss_pca.A.T + faiss_pca.b
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">A</code> is the components matrix, and <code class="language-plaintext highlighter-rouge">b</code> is a bias vector.</p> <h2 id="migrating-from-sklearn-to-faiss">Migrating from <code class="language-plaintext highlighter-rouge">sklearn</code> to <code class="language-plaintext highlighter-rouge">Faiss</code></h2> <p>To migrate from a trained <code class="language-plaintext highlighter-rouge">sklearn.PCA</code> model to a <code class="language-plaintext highlighter-rouge">faiss.PCAMatrix</code>, you need to extract:</p> <ul> <li><code class="language-plaintext highlighter-rouge">A</code>: the transformed components matrix</li> <li><code class="language-plaintext highlighter-rouge">b</code>: the bias vector to match sklearn’s behavior</li> </ul> <p>Depending on whether whitening is used:</p> <p>Without whitening:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = skl_pca.components_
b = -skl_pca.mean_ @ A.T
</code></pre></div></div> <p>With <code class="language-plaintext highlighter-rouge">whiten=True</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># escala cada fila de skl_pca.components_
A = skl_pca.components_ / sqrt(skl_pca.explained_variance_)[:, None]
b = -skl_pca.mean_ @ A.T
</code></pre></div></div> <p>After these definitions we can get:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X @ A.T + b  ==  sklearn.PCA.transform(X)
</code></pre></div></div> <h3 id="code">Code</h3> <p>Let’s create a small PCA model using the USPS digits dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">fetch_openml</span><span class="p">(</span><span class="n">data_id</span><span class="o">=</span><span class="mi">41082</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">3_000</span><span class="p">)</span>

<span class="n">skl_pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">skl_pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</code></pre></div></div> <p>Now, let’s migrate the trained model to Faiss:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">sklearn</span>
<span class="kn">import</span> <span class="n">faiss</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">sklearn_pca_to_faiss</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">faiss</span><span class="p">.</span><span class="n">PCAMatrix</span><span class="p">:</span>
    <span class="n">d_in</span> <span class="o">=</span> <span class="n">skl_pca</span><span class="p">.</span><span class="n">components_</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">d_out</span> <span class="o">=</span> <span class="n">skl_pca</span><span class="p">.</span><span class="n">n_components_</span>

    <span class="c1"># Build A: rows are components; include whitening if requested
</span>    <span class="k">if</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">,</span> <span class="sh">"</span><span class="s">whiten</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">.</span><span class="n">explained_variance_</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="n">skl_pca</span><span class="p">.</span><span class="n">components_</span> <span class="o">/</span> <span class="n">scale</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">skl_pca</span><span class="p">.</span><span class="n">components_</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">faiss_pca</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="nc">PCAMatrix</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>  <span class="c1"># eigen_power handled manually
</span>    <span class="n">faiss</span><span class="p">.</span><span class="nf">copy_array_to_vector</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="n">A</span><span class="p">)</span>

    <span class="n">mean</span> <span class="o">=</span> <span class="n">skl_pca</span><span class="p">.</span><span class="n">mean_</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">faiss</span><span class="p">.</span><span class="nf">copy_array_to_vector</span><span class="p">(</span><span class="n">mean</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="n">mean</span><span class="p">)</span>

    <span class="c1"># Choose bias so that X @ A^T + b == (X - mean) @ A^T
</span>    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="n">mean</span> <span class="o">@</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># shape (d_out,)
</span>    <span class="n">faiss</span><span class="p">.</span><span class="nf">copy_array_to_vector</span><span class="p">(</span><span class="n">b</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="n">b</span><span class="p">)</span>

    <span class="n">faiss_pca</span><span class="p">.</span><span class="n">is_trained</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="n">faiss_pca</span>

<span class="n">faiss_pca</span> <span class="o">=</span> <span class="nf">sklearn_pca_to_faiss</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">)</span>
</code></pre></div></div> <p>Important: Use Faiss’s <code class="language-plaintext highlighter-rouge">copy_array_to_vector</code> utility to load arrays into Faiss structures. See <a href="https://github.com/facebookresearch/faiss/blob/514b44fca8542bafe8640adcbf1cccce1900f74c/faiss/python/array_conversions.py#L128">this file</a> for implementation details.</p> <h3 id="validation">Validation</h3> <p>Always validate that the migration preserves results:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">faiss</span>
<span class="kn">import</span> <span class="n">sklearn</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="n">d_in</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1"># Check over some random vectors
</span><span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="nf">apply_py</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="c1"># Check over train vectors
</span><span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="nf">apply_py</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="c1"># Check over test vectors
</span><span class="n">np</span><span class="p">.</span><span class="n">testing</span><span class="p">.</span><span class="nf">assert_allclose</span><span class="p">(</span><span class="n">skl_pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="nf">apply_py</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">OK: sklearn == faiss</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># sklearn
</span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">();</span> <span class="n">_</span> <span class="o">=</span> <span class="n">skl_pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X</span><span class="p">);</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="c1"># faiss
</span><span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">();</span> <span class="n">_</span> <span class="o">=</span> <span class="n">faiss_pca</span><span class="p">.</span><span class="nf">apply_py</span><span class="p">(</span><span class="n">X</span><span class="p">);</span> <span class="n">t3</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">sklearn.transform: </span><span class="si">{</span><span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s  | </span><span class="si">{</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> vec/s</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">faiss.apply_py  : </span><span class="si">{</span><span class="p">(</span><span class="n">t3</span><span class="o">-</span><span class="n">t2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">s  | </span><span class="si">{</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">t3</span><span class="o">-</span><span class="n">t2</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> vec/s</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Speedup: </span><span class="si">{</span><span class="p">((</span><span class="n">t1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">t3</span><span class="o">-</span><span class="n">t2</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, a 1.2x speedup was achieved. See the complete code <a href="https://github.com/barufa/barufa.github.io/blob/main/assets/pca_migration.py">here</a>.</p> <h2 id="conclusion">Conclusion</h2> <p>Migrating from <code class="language-plaintext highlighter-rouge">scikit-learn</code> to <code class="language-plaintext highlighter-rouge">Faiss</code> for PCA application is a straightforward optimization with real-world impact. You can keep sklearn for training and validation, then deploy the exact same projection using Faiss—boosting inference performance without retraining.</p> <p>This method is simple, deterministic, and production-ready. And with just a few lines of code, you bridge the gap between experimentation and scalable deployment.</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="mlops,"/><category term="deployment"/><summary type="html"><![CDATA[Keep sklearn for training and validation, while leveraging Faiss for high-performance production inference.]]></summary></entry><entry><title type="html">How to Start a Machine Learning Project Before Starting a Machine Learning Project</title><link href="https://barufa.github.io/blog/2024/start-ml-project/" rel="alternate" type="text/html" title="How to Start a Machine Learning Project Before Starting a Machine Learning Project"/><published>2024-07-09T00:00:00+00:00</published><updated>2024-07-09T00:00:00+00:00</updated><id>https://barufa.github.io/blog/2024/start-ml-project</id><content type="html" xml:base="https://barufa.github.io/blog/2024/start-ml-project/"><![CDATA[<p>As someone who has made lots of mistakes in ML, I’ve come to realize the critical thinking required when approaching machine learning projects. Despite the huge potential of ML, many projects often fall short of expectations. Some reports show that only 15% of businesses’ ML projects succeed, and just 53% of AI projects make it from prototype to production.</p> <h2 id="avoid-fooling-yourself">Avoid Fooling Yourself</h2> <p>One of the most profound lessons I’ve learned in machine learning development is the temptation of solving the wrong problem. It’s surprisingly easy to succumb to biases or assumptions that lead us astray. Approaching new challenges with a healthy dose of skepticism and a critical mindset is key to ensuring we’re tackling the root issue.</p> <p>Before diving into any ML endeavor, I’ve found it extremely useful to craft a kind of “treasure map”. This map distills the project down to its essential landmarks, stripping away unnecessary complexities. Here are five pivotal questions that have helped me understand the business problem:</p> <h3 id="what-problem-are-you-solving">What Problem Are You Solving?</h3> <p>Understanding the crux of the problem is crucial. It’s easy to get caught up in crafting sophisticated models that miss the mark. For instance, in one pricing system project, the focus could be on predicting product prices to boost margins. However, if you start by creating a price prediction model that mirrors the existing pricing system, you might inadvertently perpetuate outdated practices, leading to a misaligned solution and no innovation. Always ask, “What is the true business problem we’re aiming to solve?”</p> <h3 id="what-problems-are-you-ignoring">What Problems Are You Ignoring?</h3> <p>Just as important as identifying what we’re solving is recognizing what we’re intentionally overlooking. This strategic omission helps avoid unnecessary complexity. For instance, consider a project tasked with detecting suspicious behavior in a supermarket surveillance system. As a first step, you can choose to ignore complex activity recognition models and focus solely on detecting movement, identifying people in restricted areas, or recognizing individuals who do not pass through specific zones like a cashier in a store. This approach simplifies the problem, allowing you to tackle it in manageable chunks.</p> <h3 id="who-is-your-customer-and-why-do-they-care">Who Is Your Customer and Why Do They Care?</h3> <p>In ML projects, knowing the real customer—the individual or team who will ultimately use and benefit from the solution—is vital. By cutting out intermediaries and directly understanding end-users’ needs and goals, we ensure alignment between the solution and their expectations, thereby avoiding potential miscommunications. It is crucial to understand where and how your solution will be used: What are the pain points they are facing? Will our model improve product quality or employee productivity? Does it need to be deployed on the edge or achieve real-time operation?</p> <h3 id="what-do-existing-solutions-look-like">What Do Existing Solutions Look Like?</h3> <p>Understanding current solutions provides a solid foundation and prevents reinventing the wheel. This insight can stimulate improvements and innovations.</p> <h3 id="how-do-you-measure-success">How Do You Measure Success?</h3> <p>Establishing clear success criteria is pivotal for any ML initiative. Without tangible benchmarks, it’s impossible to gauge progress or the efficacy of our solution. Success metrics should be precise and aligned with project goals. For instance, in the surveillance video project, success hinged on reducing the number of personnel needed to monitor the system, aiming for a substantial increase in productivity (where the same number of people could monitor more cameras) while maintaining quality.</p> <h2 id="embracing-iteration-and-discovery">Embracing Iteration and Discovery</h2> <p>ML projects often need a discovery phase to assess feasibility. Lasting several weeks, this phase involves delving into the problem domain, evaluating data quality and availability, and validating initial assumptions. It serves as a crucial litmus test, setting realistic expectations. By iterating through potential solutions and validating assumptions early on, you can mitigate risks and boost the odds of project success.</p> <h2 id="conclusion">Conclusion</h2> <p>Crafting effective ML systems demands a disciplined approach that circumvents common pitfalls. By leveraging these questions and adhering to fundamental principles, we navigate the intricate landscape of ML development and deliver solutions that have a meaningful impact. Remember, MLOps isn’t just about deploying models that ace test sets; it’s about building systems that operate reliably in the real world, delivering tangible value to users and organizations.</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="mlops"/><summary type="html"><![CDATA[Ever started a machine learning project, only to realize you were solving the wrong problem? Essential tips to steer your ML projects toward real impact!]]></summary></entry><entry><title type="html">DVC + Many Files: A Strategy for Efficient Large Dataset Management</title><link href="https://barufa.github.io/blog/2024/dvc-fix/" rel="alternate" type="text/html" title="DVC + Many Files: A Strategy for Efficient Large Dataset Management"/><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>https://barufa.github.io/blog/2024/dvc-fix</id><content type="html" xml:base="https://barufa.github.io/blog/2024/dvc-fix/"><![CDATA[<p>Implementing DVC at my workplace had been successful for most tasks, but dealing with a dataset containing 1 million images was tough. Uploading and downloading the dataset took several hours, so we had to use some clever tricks to streamline the workflow. In this post, I’ll share the valuable lessons we learned to help you avoid similar pitfalls.</p> <h2 id="context">Context</h2> <p>In an effort to improve data management during experiments, I decided to incorporate DVC. DVC (Data Version Control) is a tool that stores your data in remote storage and versions it using GIT. It hashes the data and saves the hash along with other metadata in source control (similar to using pointers in C). The actual data is then transferred to a corresponding folder in the remote storage.</p> <p>At first, everything worked smoothly. We could upload and download data without any issues. It was easy to track which version of our code was using which version of the data for running our experiments. However, things got complicated when we tried to work with datasets containing around 1 million images.</p> <p>It turns out that DVC struggles with handling a large number of files (&gt;200K). In our case, it took over 8 hours to upload the data with DVC and another 3 hours to download it. These times were clearly unmanageable for us. Once the download was complete, DVC needed an additional ~1.5 hours to check out the files. The conclusion was clear: we had a huge performance issue!</p> <h2 id="problem">Problem</h2> <p>DVC has to check every file to make sure it’s the right one and avoid uploading the same file twice. This computation takes time and could be inefficiently programmed. On top of the overhead per request, there’s another problem that we ran into. It costs a lot of money to fire 1 million requests against the storage account. So, we had to come up with a solution to make DVC work for us.</p> <h2 id="solution">Solution</h2> <p>The solution we decided to implement was to zip the images in several archives. That way, we could reduce the number of files DVC had to check. The easiest way to do this was to use the <code class="language-plaintext highlighter-rouge">zip</code> command in Linux. We created a script that zipped the images in groups and uploaded them to the remote storage. This way, we reduced the number of files DVC had to check from 1 million to ~1000. This change significantly improved the upload and download times.</p> <p>Here is the script we used to zip the images:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

# The directory structure should be as follows:
# imagenet/
# ├── n01440764
# │   ├── n01440764_10026.JPEG
# │   ├── n01440764_10027.JPEG
# │   ........
# ├── n01440765
# │   ├── n01440765_1000.JPEG
# │   ........
# .........

directory_path="imagenet"
max_processes=$(nproc --all)

# Function to handle ZIP operation using parallel
zip_directories() {
    find "$directory_path" -mindepth 1 -maxdepth 1 -type d | \
    parallel --progress -j "$max_processes" 'zip -qr {}.zip {} &amp;&amp; rm -rf {}'
}

# Function to handle UNZIP operation using parallel
unzip_files() {
    find "$directory_path" -type f -name "*.zip" | \
    parallel --progress -j "$max_processes" 'unzip -q {} &amp;&amp; rm {}'
}

# Main script logic
if [ $# -ne 1 ]; then
    echo "Usage: $0 [ZIP|UNZIP]"
    exit 1
fi

action=$1

case $action in
    ZIP)
        zip_directories
        ;;
    UNZIP)
        unzip_files
        ;;
    *)
        echo "Error: Unrecognized option '$action'. Usage: $0 [ZIP|UNZIP]"
        exit 1
        ;;
esac

exit 0
</code></pre></div></div> <p>This solution has some points to consider:</p> <ul> <li>I strongly recommend grouping the files in a way that makes sense for your project. In our case, we use some metadata (eg. labeling session) that naturally create a partition on the dataset. This way, we could easily navigate through the dataset and find the images we needed.</li> <li>Adding new data to the dataset should not modify the existing archives. This way, DVC will not store the same files twice. The new data should be added to a new archive. This is easy to achieve based on the metadata used to partition the dataset.</li> </ul> <p>Another solution was proposed at <a href="https://fizzylogic.nl/2023/01/13/did-you-know-dvc-doesn-t-handle-large-datasets-neither-did-we-and-here-s-how-we-fixed-it">this article</a>, which uses <code class="language-plaintext highlighter-rouge">Parquet</code> for partitioning the data instead of zipping it. This clever solution is more efficient for some cases, but it requires more effort to implement.</p> <h2 id="summary">Summary</h2> <p>DVC is a great tool for managing data in machine learning projects. However, it struggles with large datasets containing a large number of files. To overcome this limitation, we zipped the images in groups and uploaded them to the remote storage. This change significantly improved the upload and download times. I hope this post helps you avoid similar pitfalls when working with large datasets in DVC. If you have any questions or suggestions, feel free to leave a comment below. I’d love to hear from you!</p>]]></content><author><name></name></author><category term="machine-learning,"/><category term="data,"/><category term="mlops"/><summary type="html"><![CDATA[Unaware that DVC struggles with large datasets? It was also a surprise for us.]]></summary></entry></feed>